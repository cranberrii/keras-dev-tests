{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# GloVe embedding data can be found at:\n",
    "# http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# (source page: http://nlp.stanford.edu/projects/glove/)\n",
    "# 20 Newsgroup data can be found at:\n",
    "# http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model, load_model\n",
    "from keras.initializers import Constant\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_DIR = \"data/20_newsgroup/\"\n",
    "GLOVE_DIR = \"data/glove.6B/\"\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19997 texts.\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "labels_index = {}\n",
    "labels = []\n",
    "\n",
    "for name in sorted(os.listdir(NEWS_DIR)):\n",
    "    path = os.path.join(NEWS_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                if sys.version_info < (3,): \n",
    "                    f = open(fpath)\n",
    "                else:\n",
    "                    f = open(fpath, encoding='latin-1')\n",
    "                t = f.read()\n",
    "                i = t.find('\\n\\n')  # skip header\n",
    "                if 0 < i: \n",
    "                    t = t[i:]\n",
    "                texts.append(t)\n",
    "                f.close()\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "print('Found %d word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174074 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index # dict\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences[37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997, 20)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices) # shuffle!\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training emb matrix model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "print('Training emb matrix model...')\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words, EMBEDDING_DIM,\n",
    "                           embeddings_initializer=Constant(embedding_matrix),\n",
    "                           input_length=MAX_SEQUENCE_LENGTH,\n",
    "                           trainable=False)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a 1D convnet with global maxpooling\n",
    "d = 0.2\n",
    "\n",
    "seq_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_seqs = embedding_layer(seq_input)\n",
    "\n",
    "X1 = Conv1D(128, 10, activation='relu')(embedded_seqs)\n",
    "X1 = Dropout(d)(X1)\n",
    "X1 = MaxPooling1D(5)(X1)\n",
    "\n",
    "X2 = Conv1D(128, 10, activation='relu')(X1)\n",
    "X2 = Dropout(d)(X2)\n",
    "X2 = MaxPooling1D(5)(X2) #3\n",
    "\n",
    "X3 = Conv1D(128, 5, activation='relu')(X2)\n",
    "X3 = GlobalMaxPooling1D()(X3)\n",
    "\n",
    "X4 = Dense(64, activation='relu')(X3)\n",
    "preds = Dense(len(labels_index), activation='softmax')(X4)\n",
    "\n",
    "model = Model(seq_input, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 991, 128)          128128    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 991, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 198, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 189, 128)          163968    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 189, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 33, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                1300      \n",
      "=================================================================\n",
      "Total params: 2,383,700\n",
      "Trainable params: 383,700\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/10\n",
      "15998/15998 [==============================] - 169s 11ms/step - loss: 2.6208 - acc: 0.1713 - val_loss: 2.3092 - val_acc: 0.2036\n",
      "Epoch 2/10\n",
      "15998/15998 [==============================] - 165s 10ms/step - loss: 1.8681 - acc: 0.3685 - val_loss: 1.7064 - val_acc: 0.4304\n",
      "Epoch 3/10\n",
      "15998/15998 [==============================] - 149s 9ms/step - loss: 1.4864 - acc: 0.5005 - val_loss: 1.4504 - val_acc: 0.5316\n",
      "Epoch 4/10\n",
      "15998/15998 [==============================] - 152s 9ms/step - loss: 1.2758 - acc: 0.5698 - val_loss: 1.2107 - val_acc: 0.6069\n",
      "Epoch 5/10\n",
      "15998/15998 [==============================] - 170s 11ms/step - loss: 1.1130 - acc: 0.6270 - val_loss: 1.1364 - val_acc: 0.6302\n",
      "Epoch 6/10\n",
      "15998/15998 [==============================] - 165s 10ms/step - loss: 0.9871 - acc: 0.6710 - val_loss: 1.2712 - val_acc: 0.5604\n",
      "Epoch 7/10\n",
      "15998/15998 [==============================] - 144s 9ms/step - loss: 0.8855 - acc: 0.7024 - val_loss: 1.1056 - val_acc: 0.6272\n",
      "Epoch 8/10\n",
      "15998/15998 [==============================] - 147s 9ms/step - loss: 0.8030 - acc: 0.7315 - val_loss: 1.1331 - val_acc: 0.6309\n",
      "Epoch 9/10\n",
      "15998/15998 [==============================] - 149s 9ms/step - loss: 0.7428 - acc: 0.7512 - val_loss: 1.0075 - val_acc: 0.6637\n",
      "Epoch 10/10\n",
      "15998/15998 [==============================] - 153s 10ms/step - loss: 0.6650 - acc: 0.7777 - val_loss: 1.0338 - val_acc: 0.6642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f27ef9216a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model_Conv1D.h5')\n",
    "\n",
    "model.save_weights('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('model_Conv1D.h5')\n",
    "# https://stackoverflow.com/questions/42328034/keras-load-model-not-working-after-training\n",
    "model.load_weights('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {v: k for k, v in word_index.items()} # map back\n",
    "# seqs = tk.texts_to_sequences(txt1) #sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "def index_to_word(seq):\n",
    "    words = []\n",
    "    for i in seq:\n",
    "        if i != 0:\n",
    "            words.append(index_word.get(i))\n",
    "        else:\n",
    "            words.append(' ')\n",
    "    return (' '.join(words)) # output\n",
    "\n",
    "def word_to_index(seq):\n",
    "    indexes = []\n",
    "    for w in seq:\n",
    "        if w is not None:\n",
    "            indexes.append(word_index.get(w))\n",
    "        else:\n",
    "            indexes.append(' ')\n",
    "    return ((indexes)) # output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt.atheism': 0,\n",
       " 'comp.graphics': 1,\n",
       " 'comp.os.ms-windows.misc': 2,\n",
       " 'comp.sys.ibm.pc.hardware': 3,\n",
       " 'comp.sys.mac.hardware': 4,\n",
       " 'comp.windows.x': 5,\n",
       " 'misc.forsale': 6,\n",
       " 'rec.autos': 7,\n",
       " 'rec.motorcycles': 8,\n",
       " 'rec.sport.baseball': 9,\n",
       " 'rec.sport.hockey': 10,\n",
       " 'sci.crypt': 11,\n",
       " 'sci.electronics': 12,\n",
       " 'sci.med': 13,\n",
       " 'sci.space': 14,\n",
       " 'soc.religion.christian': 15,\n",
       " 'talk.politics.guns': 16,\n",
       " 'talk.politics.mideast': 17,\n",
       " 'talk.politics.misc': 18,\n",
       " 'talk.religion.misc': 19}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected category: sci.space\n",
      "Text:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             in article galaxy ucr edu ucr edu kevin marcus writes are there any public domain or shareware astronomy programs which will map out the sky at any given time and allow you to locate planets and so forth if so is there any ftp site where i can get one there are several star map programs available your job is to choose that you like try anonymous ftp from ftp funet fi pub astro pc stars pc solar mac amiga atari regards\n",
      "Prediction category: comp.graphics\n",
      "+++++\n",
      "Expected category: comp.sys.mac.hardware\n",
      "Text:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             in article cs uiuc edu cs uiuc edu terry writes got a new datadesk keyboard to go with my new centris 610 and have a problem doing desktop i hold down the command and option keys and restart but nothing happens the dip switches are set the right way and the command and option keys seem to work on anything else i'm running 7 1 btw anyone know what the problem is terry i am this message because my news program may have the first time terry i recently bought an lciii and a datadesk i don't remember trying to rebuild the desktop with it however it did give me a strange problem when i held down shift during startup to disable all extensions nothing happened i tried it with another keyboard using the same adb connector cable and it worked with the other keyboard the shift key on the datadesk keyboard worked well otherwise i checked the and they are fine try disabling your extensions and tell me if it works i am annoyed with datadesk i sent them the keyboard in the mail for inspection repair replacement the technician on the phone said they have a 10 14 day turn around time meaning you should receive the repaired keyboard in that time well they have had the keyboard for over 3 weeks and i still have gotten very little info from them about it it's annoying because it cost me 12 to send them the keyboard and their technical support line is not toll free tell me if you have a similar experience with them david mirsky mirsky gnu ai mit edu\n",
      "Prediction category: comp.os.ms-windows.misc\n",
      "+++++\n"
     ]
    }
   ],
   "source": [
    "# This code allows you to see the mislabelled examples\n",
    "# C = 5\n",
    "# y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "# X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "\n",
    "pred = model.predict(x_val)\n",
    "\n",
    "for i in range(250,255):\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != np.argmax(y_val[i])):\n",
    "        print('Expected category: %s' % list(labels_index.keys())[np.argmax(y_val[i])]) #np.argmax(y_val[i]))\n",
    "        print('Text: %s' % index_to_word(x_val[i]))    # x_val[i] + (num))\n",
    "        print('Prediction category: %s' % list(labels_index.keys())[num])\n",
    "        print('+++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: 14\n"
     ]
    }
   ],
   "source": [
    "# TODO test on new sample texts\n",
    "\n",
    "test_text = \"It still applies, except the astronomy these days is coupled to data for studies into  Earth rotation, and \\\n",
    "purturbations. Every time there is a leap second added to the New Year, remember the military and science are still co-habiting nicely.\"\n",
    "\n",
    "sequences = [[0 if word_index.get(w) is None else word_index.get(w) for w in test_text.split()]]\n",
    "sequences2 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "pred2 = model.predict(sequences2)\n",
    "print('Predicted category: %s' % np.argmax(pred2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "# ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "# print((list(char_to_ix.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO use LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
